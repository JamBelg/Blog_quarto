[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jamel Belgacem",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nBETH dataset\n\n\nApplication of various Machine Learning models on real cybersecurity data\n\n\n\n\nMachine learning\n\n\nCode\n\n\nCybersecurity\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2024\n\n\nJamel Belgacem and Papa Moryba Kouate\n\n\n\n\n\n\n  \n\n\n\n\nPicture classification (cat or dog)\n\n\n\n\n\n\n\nDeep learning\n\n\nPicture classification\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nJamel Belgacem\n\n\n\n\n\n\n  \n\n\n\n\nPicture classification (flowers)\n\n\n\n\n\n\n\nDeep learning\n\n\nPicture classification\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nJamel Belgacem\n\n\n\n\n\n\n  \n\n\n\n\nText classification\n\n\n\n\n\n\n\nDeep learning\n\n\nText classification\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nJamel Belgacem\n\n\n\n\n\n\n  \n\n\n\n\nGold origin\n\n\n\n\n\n\n\nData visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nJamel Belgacem\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Application of various Machine Learning models on real cybersecurity data (BETH dataset)",
    "section": "",
    "text": "This is a post with executable code"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Jamel Belgacem is an Application Engineer with a bachelor in electrical engineering.\nHe has a big interest in machine learning topics and data science. He has a CAS certificate in advanced machine learning from the university of Bern.\nHe is sharing personal projects and tutorials on his Github"
  },
  {
    "objectID": "posts/Beth dataset/index.html",
    "href": "posts/Beth dataset/index.html",
    "title": "BETH dataset",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Data visualization-Gold origin/Gold_origin.html",
    "href": "posts/Data visualization-Gold origin/Gold_origin.html",
    "title": "Gold origin",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(countrycode)\nlibrary(maps)\nlibrary(rworldmap)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(gridExtra)\nlibrary(DT)\nlibrary(htmlwidgets)\nlibrary(knitr)\nbg_color <- '#F2E7D5'\n\n\nThe following code fetches a CSV file from my GitHub repository and loads it into a data frame called “df”.\n\nCode# Read data from github repository\nurl <- \"https://raw.githubusercontent.com/JamBelg/Gold-origin/main/data.csv\"\ndf <- read.csv(url, sep = ',', col.names=c('Flow','Year','Period','Tarif','key','text','Land','Quantity','Value'))\n\n\n\nCode# Data preparation\n\ndf_plot <- df %>%\n  filter(Period=='_Year', \n         Year==2021, \n         text=='mined gold (according to the \"Explanatory notes\")',\n         Land!=\"_ALL\") %>%\n  select(Land,Quantity,Value)\n\n\ndf_plot$Land <- as.character(df_plot$Land)\n\n# ISO Country code\ndf_plot[df_plot$Land=='Guiana, French',1] <- 'French Guiana'\ndf_plot[df_plot$Land=='USA',1] <- \"United States of America\"\ndf_plot[df_plot$Land==\"Côte d'Ivoire\",1] <- \"Ivory Coast\"\ndf_plot[df_plot$Land=='Tanzania',1] <- \"United Republic of Tanzania\"\ndf_plot[df_plot$Land=='Hong Kong',1] <- \"Hong Kong S.A.R.\"\ndf_plot <- df_plot %>%\n  mutate(Land=countryname(Land, destination='country.name.en'))\n\n\nList of countries:\n\nCodelevels(as.factor(df_plot$Land))\n\n [1] \"Argentina\"           \"Australia\"           \"Azerbaijan\"         \n [4] \"Brazil\"              \"Burkina Faso\"        \"Canada\"             \n [7] \"Chile\"               \"Colombia\"            \"Côte d’Ivoire\"      \n[10] \"Dominican Republic\"  \"Ecuador\"             \"Finland\"            \n[13] \"France\"              \"French Guiana\"       \"Georgia\"            \n[16] \"Germany\"             \"Ghana\"               \"Guinea\"             \n[19] \"Guyana\"              \"Hong Kong SAR China\" \"Indonesia\"          \n[22] \"Kenya\"               \"Laos\"                \"Lebanon\"            \n[25] \"Liberia\"             \"Malaysia\"            \"Maldives\"           \n[28] \"Mali\"                \"Mexico\"              \"Mongolia\"           \n[31] \"Morocco\"             \"Nicaragua\"           \"Nigeria\"            \n[34] \"Norway\"              \"Oman\"                \"Panama\"             \n[37] \"Peru\"                \"Philippines\"         \"Russia\"             \n[40] \"Saudi Arabia\"        \"Senegal\"             \"South Africa\"       \n[43] \"Spain\"               \"Suriname\"            \"Sweden\"             \n[46] \"Tanzania\"            \"Thailand\"            \"United States\"      \n[49] \"Uruguay\"            \n\n\n\nCode# DT datatable\nhtmltools::tagList(datatable(df_plot))\n\n\n\n\n\n\n\nCode# Longitude & Latitude\nlibrary(maps)\nlibrary(rworldmap)\nworld_map <- getMap()\nworldmap_df <- fortify(world_map)\n\nRegions defined for each Polygons\n\nCodeworldmap_df <- worldmap_df %>%\n  left_join(.,y=df_plot,by=c(\"id\"=\"Land\"))\nworldmap_df <- worldmap_df %>%\n  mutate(color=ifelse(id=='Switzerland','#FFE15D',\n                      ifelse(id %in% df_plot$Land,\n                             ifelse(Quantity>100000,\"#8E3200\",\n                                    ifelse(Quantity>50000,\"#D1512D\",\n                                           ifelse(Quantity>10000,\"#D7A86E\",\"#FFEBC1\"))),\"White\")))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `color = ifelse(...)`.\nCaused by warning in `Ops.factor()`:\n! '>' not meaningful for factors\n\nCode# Countries shape\ncountry_shapes <- geom_polygon(data=worldmap_df[worldmap_df$id!='Antarctica',], \n                               aes(long,lat,group=group, fill=color),\n                               show.legend=FALSE, colour='black')\n\n\nas_tibble(df_plot) %>%\n  select(Land, Quantity) %>%\n  arrange(desc(Quantity)) %>%\n  top_n(20) %>%\n  tableGrob(\n    rows=NULL,\n    cols = c(\"Country\",\"Quantity [kg]\"),\n    theme=ttheme_default(\n      base_size = 5,\n      core=list(\n        fg_params=list(\n          # fontfamily=font_rc,\n          col=c(rep(\"#8E3200\",3),rep(\"#D1512D\",3),rep(\"#D7A86E\",14)),\n          hjust=c(rep(0,20), rep(0.5,20)),\n          x=c(rep(0.05,20), rep(.5,20))\n        )\n      )\n    )\n  ) ->tbl\n\nSelecting by Quantity\n\nCodelibrary(gridExtra)\nlibrary(grid)\ntitle <- textGrob(expression(bold(underline(\"Top 20 (by quantity) Mining gold origin\"))),\n                  y=0.95,vjust=0.5, gp=gpar(fontsize=10, fontface='bold'))\n                                            \ngt <- gTree(children=gList(tbl,title))\n\n# Plot\nggplot()+\n  country_shapes+scale_fill_identity()+\n  coord_equal()+\n  theme_dark()+\n  theme(\n    # TITLE\n    plot.title.position = \"plot\",\n    plot.title = element_text(\n                              #family = title_font,\n                              face = \"bold\",\n                              color = \"black\",\n                              size = 20,\n                              lineheight = 1,\n                              hjust = 0.5,\n                              margin = margin(20,0,20,0)),\n    plot.subtitle = element_text(\n                                 #family = title_font,\n                                 face = \"bold\",\n                                 color = \"black\",\n                                 size = 15,\n                                 lineheight = 1,\n                                 hjust = 0.5),\n    # CAPTION\n    plot.caption=element_text(color=\"#393E46\",\n                              # family = title_font,\n                              size=12,\n                              hjust=1),\n    axis.text=element_blank(),\n    axis.title = element_blank()\n  )+\n  labs(\n    title=\"Origin of the mining gold refined in Switzerland\",\n    subtitle=\"Year 2021\",\n    caption=\"Source: Swiss federal office for customs and border security\"\n  )+\n  annotation_custom(gt, xmin = -50, xmax = 0, ymin = 50, ymax=0) #, xmin = -16920565, xmax = -14000000,  ymin=761378/2.25, ymax = 761378\n\n\n\n\n\n(a) Geo plot\n\n\n\n\nFigure 1: Charts"
  },
  {
    "objectID": "posts/Image classification-Flowers/Flower pictures classification.html",
    "href": "posts/Image classification-Flowers/Flower pictures classification.html",
    "title": "Picture classification (flowers)",
    "section": "",
    "text": "Another application of deep learning is the image classification.\nIn this tutorial I will train a keras network to predict the class of the flower based on the picture.\n\nImport libraries\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Conv2D, MaxPooling2D, Dense\nfrom keras.optimizers import Adam\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\n\n\nRead data\nThe data can be downloaded from Kaggle.\nThe dataset consists of 5 different flower classes: Lilly, Lotus, Sunflower, Orchid and Tulip. Each flower images are stored in one folder (1000 images)\n\n\nCode\ndata_path = 'flower_images'\nos.listdir(data_path)[1:]\n\n\n['Lotus', 'Tulip', 'Orchid', 'Lilly', 'Sunflower']\n\n\nPictures are resized to 224,224 pixels and transformed to array.\nPixels are stored in data list, labels contains the flower class (folder’s name)\n\n\nCode\ndata_path = 'flower_images'\ndata=[]\nlabels=[]\n\nfor label, folder in enumerate(os.listdir(data_path)[1:]):\n    folder_path = os.path.join(data_path,folder)\n    for file_name in os.listdir(folder_path):\n        picture_path = os.path.join(folder_path,file_name)\n        image = load_img(picture_path, target_size=(224,224))\n        image = img_to_array(image)\n        data.append(image)\n        labels.append(label)\n\ndata = np.array(data)\nlabel_class =labels\nlabels = np.array(labels)\n\n\n\nSplit data into train and test\nThis line of code splits the ‘data’ and ‘labels’ into training and testing sets, with a test size of 20%, ensuring that 80% of the data is used for training.\nBy using the ‘train_test_split’ function from scikit-learn, this code efficiently partitions the ‘data’ and ‘labels’ into ‘x_train’, ‘x_test’, ‘y_train’, and ‘y_test’ variables, facilitating the machine learning workflow.\nThe ‘random_state=42’ parameter ensures reproducibility of the split, meaning that every time the code runs with the same ‘random_state’ value, it will produce the same data split, which is beneficial for result consistency and debugging.\n\n\nCode\nx_train, x_test, y_train, y_test =train_test_split(data, labels, test_size=0.2, random_state=42)\n\n\n\n\nCrate the neural networks\nIn this model, I will use a CNN architecture with four convolutional layers, each followed by max-pooling, and two fully connected layers for classification. It is designed to process images with dimensions 224x224 pixels and three color channels (RGB).\n\n\nCode\n# building CNN\nmodel = Sequential([\n    # Conv layer 1:\n    Conv2D(32, (3, 3), input_shape=(224,224,3), activation='relu'),\n    MaxPooling2D(pool_size = (2, 2)),\n    \n    # Conv layer 2:\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size = (2, 2)),\n    \n    # Conv layer 3:\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size = (2, 2)),\n    \n    # Conv layer 4:\n    Conv2D(256, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size = (2, 2)),\n    \n    Flatten(),\n    \n    # fully connected layers:\n    Dense(units = 512, activation = 'relu'),\n    Dense(units = 5, activation = 'softmax')\n    \n])\n\nmodel.compile(optimizer = Adam(learning_rate=0.001), loss = 'sparse_categorical_crossentropy', metrics ='accuracy')\nmodel.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 222, 222, 32)      896       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n 2D)                                                             \n                                                                 \n conv2d_3 (Conv2D)           (None, 24, 24, 256)       295168    \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 12, 12, 256)      0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 36864)             0         \n                                                                 \n dense (Dense)               (None, 512)               18874880  \n                                                                 \n dense_1 (Dense)             (None, 5)                 2565      \n                                                                 \n=================================================================\nTotal params: 19,265,861\nTrainable params: 19,265,861\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nFit the model\nThe next code is a crucial step where we train the “model” using the training data “x_train” and corresponding labels “y_train.”.\nBy setting a validation split of 20%, we can monitor how well the model generalizes on unseen data during training.\nThe batch size is set to 32, and we train the model for 10 epochs, allowing it to learn from the data in multiple passes.\nThe “verbose=1” parameter provides a progress bar, so we can easily track the training process.\n\n\nCode\nhistory = model.fit(x_train, y_train, validation_split=0.2, batch_size=32, epochs=10, verbose=1)\n\n\nEpoch 1/10\n\n\n2023-07-31 08:55:10.479379: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n\n\n100/100 [==============================] - 40s 401ms/step - loss: 20.4331 - accuracy: 0.2750 - val_loss: 1.5736 - val_accuracy: 0.2788\nEpoch 2/10\n100/100 [==============================] - 40s 399ms/step - loss: 1.4439 - accuracy: 0.3950 - val_loss: 1.3809 - val_accuracy: 0.4600\nEpoch 3/10\n100/100 [==============================] - 40s 397ms/step - loss: 1.2416 - accuracy: 0.4928 - val_loss: 1.3537 - val_accuracy: 0.4675\nEpoch 4/10\n100/100 [==============================] - 40s 397ms/step - loss: 1.0039 - accuracy: 0.6059 - val_loss: 1.2419 - val_accuracy: 0.5500\nEpoch 5/10\n100/100 [==============================] - 40s 397ms/step - loss: 0.7695 - accuracy: 0.7022 - val_loss: 1.2497 - val_accuracy: 0.5813\nEpoch 6/10\n100/100 [==============================] - 40s 398ms/step - loss: 0.5321 - accuracy: 0.7941 - val_loss: 1.7166 - val_accuracy: 0.5875\nEpoch 7/10\n100/100 [==============================] - 39s 395ms/step - loss: 0.4016 - accuracy: 0.8644 - val_loss: 1.8834 - val_accuracy: 0.6438\nEpoch 8/10\n100/100 [==============================] - 40s 396ms/step - loss: 0.3173 - accuracy: 0.8913 - val_loss: 1.8827 - val_accuracy: 0.6750\nEpoch 9/10\n100/100 [==============================] - 40s 397ms/step - loss: 0.2226 - accuracy: 0.9284 - val_loss: 2.1163 - val_accuracy: 0.6275\nEpoch 10/10\n100/100 [==============================] - 40s 398ms/step - loss: 0.2384 - accuracy: 0.9250 - val_loss: 2.2468 - val_accuracy: 0.6425\n\n\n\n\nCode\n# Get the training and validation accuracy\ntraining_accuracy = history.history['accuracy']\nvalidation_accuracy = history.history['val_accuracy']\n\n# Get the training and validation loss\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\n# Plot the accuracy\nplt.figure(figsize=(8, 4))\nplt.plot(training_accuracy, label='Training Accuracy')\nplt.plot(validation_accuracy, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n# Plot the loss\nplt.figure(figsize=(8, 4))\nplt.plot(training_loss, label='Training Loss')\nplt.plot(validation_loss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAccuracy\nThis model is not giving good accuracy as you can see in the next code:\n\n\nCode\ny_prediction = np.argmax(model.predict(x_test), axis=1)\nacc = 100*accuracy_score(y_true=y_test, y_pred=y_prediction)\nprint(f\"Accuracy of the model: {acc:.1f} %\")\n\n\n32/32 [==============================] - 3s 104ms/step\nAccuracy of the model: 66.8 %\n\n\n\n\nCode\nconf_matrix = confusion_matrix(y_true=y_test, y_pred=y_prediction)\nprint(conf_matrix)\n\n\n[[148  13  18  26  11]\n [ 16 124  12  23  15]\n [ 32  33  94  25   8]\n [ 29  24   7 127  23]\n [  0   4   6   7 175]]\n\n\n\n\nCode\nclass_labels = ['Lilly', 'Lotus', 'Orchid', 'Sunflower', 'Tulip']\nConfusionMatrixDisplay(conf_matrix, display_labels=class_labels).plot()\n\n\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2a6461f00>\n\n\n\n\n\nThis model is not very accurate, a lot of pictures are predicted as Lotus type.\n\n\n\nVGG16\nVGG16 is a pre-trained convolutional neural network architecture that was introduced as part of the Visual Geometry Group’s participation in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014.\nIt consists of 16 layers, including 13 convolutional layers and 3 fully connected layers, making it deeper than previous models. The network is known for its simplicity and effectiveness in image classification tasks, and its pre-trained weights can be leveraged for transfer learning, allowing developers to use it as a powerful feature extractor for a wide range of visual recognition tasks.\n\nExtract features\nThis code defines and uses the VGG16 model pre-trained on ImageNet for feature extraction from flower images in the ‘flower_images’ directory.\nThe function ‘extract_features’ takes a generator, ‘train_generator’, and the number of samples (train_sample_count) as inputs, then it uses the VGG16 model to predict features for each batch of images, storing the extracted features and corresponding labels in ‘train_features’ and ‘train_labels’ arrays, respectively.\nThe ImageDataGenerator is used to preprocess the images, and the specified batch size is 32 for the feature extraction process.\n\n\nCode\ntrain_dir = 'flower_images'\nvgg16file = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nvgg16 =  VGG16(include_top=False, input_shape=(224,224,3), weights=vgg16file)\n\n# Extract features from images using VGG16 model\ndef extract_features(generator, sample_count):\n    features = np.zeros(shape=(sample_count, 7, 7, 512))\n    labels = np.zeros(shape=(sample_count))\n    i = 0\n    for inputs_batch, labels_batch in generator:\n        features_batch = vgg16.predict(inputs_batch)\n        features[i*32:(i+1)*32] = features_batch\n        labels[i*32:(i+1)*32] = np.argmax(labels_batch, axis=1)\n        i+=1\n        if i*32>=sample_count:\n            break\n    return features, labels\n\n# Specify the directory and the number of samples for feature extraction\ngenerator = ImageDataGenerator(rescale = 1./255)\ntrain_generator = generator.flow_from_directory(train_dir, target_size=(224,224), batch_size=32, class_mode='categorical', shuffle=True)\ntrain_sample_count = 5000\nfeatures, labels = extract_features(train_generator, train_sample_count)\n# Split data into train and test sets (80% train, 20% test)\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)\n\n\nFound 5000 images belonging to 5 classes.\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 0s 463ms/step\n\n\n\n\nDefine and fit the model\nThis code defines a sequential neural network model (model4) with a Flatten layer, followed by two Dense layers (128 units with ReLU activation and 5 units with softmax activation) for a multi-class classification task.\nThe model is compiled with the Adam optimizer, sparse categorical cross-entropy loss, and accuracy metric.\nTwo callbacks, ModelCheckpoint and EarlyStopping, are specified to save the best model weights based on validation accuracy and to stop training early if validation accuracy does not improve for 20 consecutive epochs.\n\n\nCode\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nmodel4 = Sequential([\n    Flatten(input_shape = (7,7,512)),\n    Dense(128, activation='relu'),\n    Dense(5, activation='softmax')\n])\n\nmodel4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ncheckpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_accuracy', verbose=1,save_best_only=True,\n                             save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\nhistory4 = model4.fit(train_features, train_labels, epochs=20, validation_split=0.2, callbacks=[checkpoint, early])\n\n\nWARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n\n\nEpoch 1/20\n 97/100 [============================>.] - ETA: 0s - loss: 1.0649 - accuracy: 0.6627\nEpoch 1: val_accuracy improved from -inf to 0.82125, saving model to vgg16_1.h5\n100/100 [==============================] - 1s 8ms/step - loss: 1.0469 - accuracy: 0.6678 - val_loss: 0.5366 - val_accuracy: 0.8213\nEpoch 2/20\n 93/100 [==========================>...] - ETA: 0s - loss: 0.2854 - accuracy: 0.9123\nEpoch 2: val_accuracy improved from 0.82125 to 0.84625, saving model to vgg16_1.h5\n100/100 [==============================] - 1s 8ms/step - loss: 0.2818 - accuracy: 0.9134 - val_loss: 0.4852 - val_accuracy: 0.8462\nEpoch 3/20\n 94/100 [===========================>..] - ETA: 0s - loss: 0.1447 - accuracy: 0.9691\nEpoch 3: val_accuracy improved from 0.84625 to 0.89250, saving model to vgg16_1.h5\n100/100 [==============================] - 1s 8ms/step - loss: 0.1447 - accuracy: 0.9697 - val_loss: 0.3607 - val_accuracy: 0.8925\nEpoch 4/20\n 96/100 [===========================>..] - ETA: 0s - loss: 0.0695 - accuracy: 0.9912\nEpoch 4: val_accuracy improved from 0.89250 to 0.90125, saving model to vgg16_1.h5\n100/100 [==============================] - 1s 7ms/step - loss: 0.0689 - accuracy: 0.9916 - val_loss: 0.3328 - val_accuracy: 0.9013\nEpoch 5/20\n 98/100 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9927\nEpoch 5: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 8ms/step - loss: 0.0513 - accuracy: 0.9925 - val_loss: 0.3466 - val_accuracy: 0.8975\nEpoch 6/20\n 96/100 [===========================>..] - ETA: 0s - loss: 0.0312 - accuracy: 0.9971\nEpoch 6: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 7ms/step - loss: 0.0325 - accuracy: 0.9966 - val_loss: 0.3546 - val_accuracy: 0.9000\nEpoch 7/20\n 96/100 [===========================>..] - ETA: 0s - loss: 0.0313 - accuracy: 0.9958\nEpoch 7: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 7ms/step - loss: 0.0312 - accuracy: 0.9959 - val_loss: 0.3492 - val_accuracy: 0.8988\nEpoch 8/20\n 99/100 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9972\nEpoch 8: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 7ms/step - loss: 0.0239 - accuracy: 0.9972 - val_loss: 0.3576 - val_accuracy: 0.8938\nEpoch 9/20\n100/100 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9975\nEpoch 9: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 8ms/step - loss: 0.0163 - accuracy: 0.9975 - val_loss: 0.3921 - val_accuracy: 0.8975\nEpoch 10/20\n 98/100 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9984\nEpoch 10: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 7ms/step - loss: 0.0193 - accuracy: 0.9984 - val_loss: 0.4618 - val_accuracy: 0.8875\nEpoch 11/20\n 96/100 [===========================>..] - ETA: 0s - loss: 0.0186 - accuracy: 0.9977\nEpoch 11: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 7ms/step - loss: 0.0189 - accuracy: 0.9975 - val_loss: 0.4652 - val_accuracy: 0.8788\nEpoch 12/20\n 96/100 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.9977\nEpoch 12: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 7ms/step - loss: 0.0186 - accuracy: 0.9978 - val_loss: 0.4095 - val_accuracy: 0.8963\nEpoch 13/20\n 95/100 [===========================>..] - ETA: 0s - loss: 0.0209 - accuracy: 0.9970\nEpoch 13: val_accuracy did not improve from 0.90125\n100/100 [==============================] - 1s 7ms/step - loss: 0.0215 - accuracy: 0.9969 - val_loss: 0.4301 - val_accuracy: 0.8763\nEpoch 14/20\n 94/100 [===========================>..] - ETA: 0s - loss: 0.0155 - accuracy: 0.9983\nEpoch 14: val_accuracy improved from 0.90125 to 0.90625, saving model to vgg16_1.h5\n100/100 [==============================] - 1s 7ms/step - loss: 0.0147 - accuracy: 0.9984 - val_loss: 0.3729 - val_accuracy: 0.9062\nEpoch 15/20\n 94/100 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.9973\nEpoch 15: val_accuracy did not improve from 0.90625\n100/100 [==============================] - 1s 7ms/step - loss: 0.0137 - accuracy: 0.9975 - val_loss: 0.4334 - val_accuracy: 0.8925\nEpoch 16/20\n 95/100 [===========================>..] - ETA: 0s - loss: 0.0272 - accuracy: 0.9957\nEpoch 16: val_accuracy did not improve from 0.90625\n100/100 [==============================] - 1s 7ms/step - loss: 0.0260 - accuracy: 0.9959 - val_loss: 0.4587 - val_accuracy: 0.8925\nEpoch 17/20\n 93/100 [==========================>...] - ETA: 0s - loss: 0.0234 - accuracy: 0.9953\nEpoch 17: val_accuracy did not improve from 0.90625\n100/100 [==============================] - 1s 7ms/step - loss: 0.0223 - accuracy: 0.9956 - val_loss: 0.4478 - val_accuracy: 0.8850\nEpoch 18/20\n 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9959\nEpoch 18: val_accuracy did not improve from 0.90625\n100/100 [==============================] - 1s 7ms/step - loss: 0.0305 - accuracy: 0.9959 - val_loss: 0.5112 - val_accuracy: 0.8813\nEpoch 19/20\n 96/100 [===========================>..] - ETA: 0s - loss: 0.0095 - accuracy: 0.9984\nEpoch 19: val_accuracy did not improve from 0.90625\n100/100 [==============================] - 1s 7ms/step - loss: 0.0113 - accuracy: 0.9978 - val_loss: 0.4386 - val_accuracy: 0.8900\nEpoch 20/20\n 94/100 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.9977\nEpoch 20: val_accuracy did not improve from 0.90625\n100/100 [==============================] - 1s 7ms/step - loss: 0.0137 - accuracy: 0.9978 - val_loss: 0.5293 - val_accuracy: 0.8775\n\n\n\n\nCode\n# Get the training and validation accuracy\ntraining_accuracy = history4.history['accuracy']\nvalidation_accuracy = history4.history['val_accuracy']\n\n# Get the training and validation loss\ntraining_loss = history4.history['loss']\nvalidation_loss = history4.history['val_loss']\n\n# Plot the accuracy\nplt.figure(figsize=(8, 4))\nplt.plot(training_accuracy, label='Training Accuracy')\nplt.plot(validation_accuracy, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n# Plot the loss\nplt.figure(figsize=(8, 4))\nplt.plot(training_loss, label='Training Loss')\nplt.plot(validation_loss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAccuracy of the model\nThis code calculates the predicted class labels (‘y_prediction’) using the trained ‘model4’ on the ‘test_features’ data and then computes the accuracy score by comparing the predicted labels with the true labels (‘test_labels’)\n\n\nCode\ny_prediction = np.argmax(model4.predict(test_features), axis=1)\nacc = accuracy_score(y_true=test_labels, y_pred=y_prediction)\nprint(f\"Accuracy of the model using VGG16: {(acc*100):.2f} %\")\n\n\n32/32 [==============================] - 0s 1ms/step\nAccuracy of the model using VGG16: 89.10 %\n\n\n\nUsing VGG16 as a feature extraction model significantly boosts the accuracy of the overall model, as evidenced by the improved performance when predicting on the ‘test_features’ data compared to a traditional approach without leveraging the VGG16 features.\n\nThis code downloads an image (Tulip) from a specified URL into a temporary folder.\nThe ‘model4’ predicts the class probabilities for the new image, and the label with the highest probability is determined as the predicted class (Tulip).\n\n\nCode\n# Download picture from URL on temporary folder\nimport urllib.request\nimport tempfile\n\nurl = \"https://images.all-free-download.com/images/graphicwebp/beautiful_tulip_199087.webp\"\ntempfile_path =tempfile.mktemp()\nurllib.request.urlretrieve(url, tempfile_path)\n\n\nnew_image = load_img(tempfile_path, target_size=(224,224))\n\nim=plt.imshow(np.asarray(new_image))\nim\n\nclass_labels = ['Lilly', 'Lotus', 'Orchid', 'Sunflower', 'Tulip']\n\nnew_image = img_to_array(new_image)\nnew_image = np.expand_dims(new_image, axis=0)\nnew_image = new_image / 255.0  # Normalize the image\n\n# Extract features from the new image using VGG16 model\nnew_features = vgg16.predict(new_image)\n\n# Make prediction on the new image\nprediction = model4.predict(new_features)\nprint(prediction)\npredicted_label = np.argmax(prediction)\nprint(predicted_label)\nprint(\"Predicted label:\", class_labels[predicted_label])\n\n\n1/1 [==============================] - 0s 84ms/step\n1/1 [==============================] - 0s 9ms/step\n[[1.8882914e-01 1.9313225e-05 2.3650099e-02 8.2870019e-06 7.8749317e-01]]\n4\nPredicted label: Tulip\n\n\n\n\n\n\n\n\nConclusion\nIn conclusion, incorporating VGG16 as a feature extraction model has proven to be highly effective in improving the accuracy of image predictions.\nBy leveraging the powerful pre-trained VGG16 architecture, the model gains the ability to capture intricate patterns and high-level features from images, leading to more precise and reliable predictions. This makes it a valuable choice for various image recognition and classification tasks, providing a solid foundation for achieving superior results in the domain of computer vision."
  },
  {
    "objectID": "posts/Picture classification/Cats and dogs.html",
    "href": "posts/Picture classification/Cats and dogs.html",
    "title": "Picture classification (cat or dog)",
    "section": "",
    "text": "Introduction\nDeep learning is a subset of artificial intelligence that involves training neural networks to learn and recognize patterns from vast amounts of data. One of its prominent applications is image classification, where deep learning models can automatically identify objects and features within images.\nIn this tutorial I will use a keras neural network to try to predict the class of a picture (Cat / Dog).\nYou can download the dataset from Kaggle.\nKeras is an open-source deep learning framework that provides a user-friendly interface to build, train, and deploy neural networks. It is designed to be simple and intuitive, making it an ideal choice for beginners and researchers alike.\n\nImport libraries\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Keras\nfrom tensorflow import keras\nfrom keras.utils import image_dataset_from_directory\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import load_img, img_to_array\n\n# Pre-trained VGG16 model\nfrom keras.applications.vgg16 import VGG16\n\n# Confusion matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n# Download picture from URL on temporary folder\nimport urllib.request\nimport tempfile\n\n\n\n\nRead data\nThe dataset is composed from from two folders (train and test), and each of this folder contains pictures already classed in two folders (cats and dogs)\nTo read the data, we’ll use the function image_dataset_from_directory of Keras.\nWe give simply the path of the pictures and it generates a tensorflow datset.\nSome arguments can be passed as the picture size (by defalut 256,256) and the size of the batches.\n\n\nCode\ntrain_dir = 'data/train'\ntest_dir = 'data/test'\n\nseed_train_validation = 1\n\ntrain_dataset = image_dataset_from_directory(train_dir, image_size=(128, 128), batch_size=32, validation_split=0.2, subset='training', seed=seed_train_validation)\nvalidation_dataset = image_dataset_from_directory(train_dir, image_size=(128, 128), batch_size=32, validation_split=0.2, subset='validation', seed=seed_train_validation)\ntest_dataset = image_dataset_from_directory(test_dir, image_size=(128, 128), batch_size=32)\n\n\nFound 557 files belonging to 2 classes.\nUsing 446 files for training.\nFound 557 files belonging to 2 classes.\nUsing 111 files for validation.\nFound 140 files belonging to 2 classes.\n\n\n\n\nCode\n\n# building neural networks\nmodel = keras.Sequential([\n    # Conv layer 1:\n    keras.layers.Conv2D(256, (3, 3), input_shape=(128,128,3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(pool_size = (2, 2)),\n    \n    # Conv layer 2:\n    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(pool_size = (2, 2)),\n    \n    # Conv layer 3:\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(pool_size = (2, 2)),\n    \n    keras.layers.Flatten(),\n    \n    # fully connected layers:\n    keras.layers.Dense(units = 128, activation = 'relu'),\n    keras.layers.Dense(units =1, activation = 'sigmoid')\n    \n])\n\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate=0.0001), loss = 'binary_crossentropy', metrics ='accuracy')\n\nmodel.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 126, 126, 256)     7168      \n                                                                 \n batch_normalization (BatchN  (None, 126, 126, 256)    1024      \n ormalization)                                                   \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 63, 63, 256)      0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 61, 61, 128)       295040    \n                                                                 \n batch_normalization_1 (Batc  (None, 61, 61, 128)      512       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 30, 30, 128)      0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 28, 28, 64)        73792     \n                                                                 \n batch_normalization_2 (Batc  (None, 28, 28, 64)       256       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 12544)             0         \n                                                                 \n dense (Dense)               (None, 128)               1605760   \n                                                                 \n dense_1 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,983,681\nTrainable params: 1,982,785\nNon-trainable params: 896\n_________________________________________________________________\n\n\n\n\nCode\nlogs = model.fit(train_dataset, epochs=10, validation_data=validation_dataset,validation_steps=2000/32)\n\n\nEpoch 1/10\n\n\n2023-07-24 16:15:23.623191: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n\n\n14/14 [==============================] - ETA: 0s - loss: 1.0300 - accuracy: 0.5426WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 62.5 batches). You may need to use the repeat() function when building your dataset.\n14/14 [==============================] - 12s 787ms/step - loss: 1.0300 - accuracy: 0.5426 - val_loss: 0.8979 - val_accuracy: 0.5495\nEpoch 2/10\n14/14 [==============================] - 10s 681ms/step - loss: 0.3244 - accuracy: 0.8812\nEpoch 3/10\n14/14 [==============================] - 10s 683ms/step - loss: 0.1425 - accuracy: 0.9821\nEpoch 4/10\n14/14 [==============================] - 10s 680ms/step - loss: 0.0709 - accuracy: 1.0000\nEpoch 5/10\n14/14 [==============================] - 10s 680ms/step - loss: 0.0391 - accuracy: 1.0000\nEpoch 6/10\n14/14 [==============================] - 10s 684ms/step - loss: 0.0279 - accuracy: 1.0000\nEpoch 7/10\n14/14 [==============================] - 10s 696ms/step - loss: 0.0192 - accuracy: 1.0000\nEpoch 8/10\n14/14 [==============================] - 10s 688ms/step - loss: 0.0161 - accuracy: 1.0000\nEpoch 9/10\n14/14 [==============================] - 10s 696ms/step - loss: 0.0123 - accuracy: 1.0000\nEpoch 10/10\n14/14 [==============================] - 10s 690ms/step - loss: 0.0104 - accuracy: 1.0000\n\n\n\n\nCode\nplt.title('Training Log')\nplt.plot(logs.history['loss'], label='Training Loss')\nplt.plot(logs.history['accuracy'], label='Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Score')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nres = model.evaluate(test_dataset)\naccuracy = res[1]\nprint(\"Accuracy [perc]: %.1f\" %(accuracy*100))\n\n\n5/5 [==============================] - 1s 160ms/step - loss: 0.8013 - accuracy: 0.5929\nAccuracy [perc]: 59.3\n\n\nThe model’s low accuracy may be attributed to underfitting, where it fails to capture the complexity of the data, resulting in poor generalization. Alternatively, it could be suffering from overfitting, where the model becomes too specific to the training data and performs poorly on unseen examples.\n\n\nwith VGG16\nVGG16 is a pre-trained convolutional neural network architecture that was introduced as part of the Visual Geometry Group’s participation in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014.\nIt consists of 16 layers, including 13 convolutional layers and 3 fully connected layers, making it deeper than previous models. The network is known for its simplicity and effectiveness in image classification tasks, and its pre-trained weights can be leveraged for transfer learning, allowing developers to use it as a powerful feature extractor for a wide range of visual recognition tasks.\n\n\nCode\ntrain_dir = 'data/train'\ntest_dir = 'data/test'\n\nvgg16file = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nvgg16 =  VGG16(include_top=False, input_shape=(224,224,3), weights=vgg16file)\n\n# Extract features from images using VGG16 model\ndef extract_features(generator, sample_count):\n    features = np.zeros(shape=(sample_count, 7, 7, 512))\n    labels = np.zeros(shape=(sample_count))\n    i = 0\n    for inputs_batch, labels_batch in generator:\n        features_batch = vgg16.predict(inputs_batch)\n        features[i*32:(i+1)*32] = features_batch\n        labels[i*32:(i+1)*32] = np.argmax(labels_batch, axis=1)\n        i+=1\n        if i*32>=sample_count:\n            break\n    return features, labels\n\n# Specify the directory and the number of samples for feature extraction\ngenerator = ImageDataGenerator(rescale = 1./255)\ntrain_generator = generator.flow_from_directory(train_dir, target_size=(224,224), batch_size=32, class_mode='categorical', shuffle=True)\ntrain_sample_count = 557\ntrain_features, train_labels = extract_features(train_generator, train_sample_count)\ntest_generator = generator.flow_from_directory(test_dir, target_size=(224,224), batch_size=32, class_mode='categorical', shuffle=True)\ntest_sample_count = 140\ntest_features, test_labels = extract_features(test_generator, test_sample_count)\n\n\nFound 557 images belonging to 2 classes.\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 1s 706ms/step\nFound 140 images belonging to 2 classes.\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 1s 621ms/step\n\n\n\n\nCode\nmodel2 = Sequential([\n    Flatten(input_shape = (7,7,512)),\n    Dense(128, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\nmodel2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ncheckpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_accuracy', verbose=1,save_best_only=True,\n                             save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\nhistory2 = model2.fit(train_features, train_labels, epochs=20, validation_split=0.2, callbacks=[checkpoint, early])\n\n\nWARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\nEpoch 1/20\n11/14 [======================>.......] - ETA: 0s - loss: 3.6519 - accuracy: 0.5057\nEpoch 1: val_accuracy improved from -inf to 0.67857, saving model to vgg16_1.h5\n14/14 [==============================] - 0s 12ms/step - loss: 3.1537 - accuracy: 0.5236 - val_loss: 0.5144 - val_accuracy: 0.6786\nEpoch 2/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.5746 - accuracy: 0.7159\nEpoch 2: val_accuracy improved from 0.67857 to 0.71429, saving model to vgg16_1.h5\n14/14 [==============================] - 0s 8ms/step - loss: 0.5270 - accuracy: 0.7461 - val_loss: 0.5808 - val_accuracy: 0.7143\nEpoch 3/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.2592 - accuracy: 0.8807\nEpoch 3: val_accuracy improved from 0.71429 to 0.84821, saving model to vgg16_1.h5\n14/14 [==============================] - 0s 7ms/step - loss: 0.2385 - accuracy: 0.8966 - val_loss: 0.2800 - val_accuracy: 0.8482\nEpoch 4/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.1334 - accuracy: 0.9631\nEpoch 4: val_accuracy improved from 0.84821 to 0.96429, saving model to vgg16_1.h5\n14/14 [==============================] - 0s 8ms/step - loss: 0.1269 - accuracy: 0.9685 - val_loss: 0.1926 - val_accuracy: 0.9643\nEpoch 5/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0809 - accuracy: 0.9915\nEpoch 5: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0832 - accuracy: 0.9933 - val_loss: 0.2033 - val_accuracy: 0.9375\nEpoch 6/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0680 - accuracy: 0.9972\nEpoch 6: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0645 - accuracy: 0.9978 - val_loss: 0.2273 - val_accuracy: 0.9018\nEpoch 7/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0501 - accuracy: 0.9972\nEpoch 7: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.9978 - val_loss: 0.1884 - val_accuracy: 0.9554\nEpoch 8/20\n12/14 [========================>.....] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\nEpoch 8: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.1847 - val_accuracy: 0.9464\nEpoch 9/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000\nEpoch 9: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9554\nEpoch 10/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000\nEpoch 10: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.1901 - val_accuracy: 0.9554\nEpoch 11/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\nEpoch 11: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.1884 - val_accuracy: 0.9464\nEpoch 12/20\n10/14 [====================>.........] - ETA: 0s - loss: 0.0212 - accuracy: 1.0000\nEpoch 12: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9554\nEpoch 13/20\n10/14 [====================>.........] - ETA: 0s - loss: 0.0174 - accuracy: 1.0000\nEpoch 13: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9464\nEpoch 14/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\nEpoch 14: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.1906 - val_accuracy: 0.9464\nEpoch 15/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000\nEpoch 15: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.1883 - val_accuracy: 0.9286\nEpoch 16/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\nEpoch 16: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.2001 - val_accuracy: 0.9464\nEpoch 17/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\nEpoch 17: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.1912 - val_accuracy: 0.9375\nEpoch 18/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\nEpoch 18: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1988 - val_accuracy: 0.9375\nEpoch 19/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0099 - accuracy: 1.0000\nEpoch 19: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 7ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.1931 - val_accuracy: 0.9375\nEpoch 20/20\n11/14 [======================>.......] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\nEpoch 20: val_accuracy did not improve from 0.96429\n14/14 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9375\n\n\n\n\nCode\n\n# Get the training and validation accuracy\ntraining_accuracy = history2.history['accuracy']\nvalidation_accuracy = history2.history['val_accuracy']\n\n# Get the training and validation loss\ntraining_loss = history2.history['loss']\nvalidation_loss = history2.history['val_loss']\n\n# Plot the accuracy\nplt.figure(figsize=(8, 4))\nplt.plot(training_accuracy, label='Training Accuracy')\nplt.plot(validation_accuracy, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n# Plot the loss\nplt.figure(figsize=(8, 4))\nplt.plot(training_loss, label='Training Loss')\nplt.plot(validation_loss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\ny_prediction = np.argmax(model2.predict(test_features), axis=1)\nacc = accuracy_score(y_true=test_labels, y_pred=y_prediction)\nprint(\"Accuracy: %.2f\" %(acc*100))\n\n\n5/5 [==============================] - 0s 1ms/step\nAccuracy: 79.29\n\n\nAs you see, applying VGG16 as a pre-trained model has improved the accuracy due to its deep architecture and extensive feature extraction capabilities.\nThe network’s pre-trained weights, learned from a large-scale dataset, enable it to recognize complex patterns and features, making it highly effective in various visual recognition scenarios.\n\nConfusion matrix\n\n\nCode\nconf_matrix = confusion_matrix(y_true=test_labels, y_pred=y_prediction)\nclass_labels = ['Cats', 'Dogs']\nConfusionMatrixDisplay(conf_matrix, display_labels=class_labels).plot()\n\n\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29349f400>\n\n\n\n\n\n\n\n\nSave models\n\n\nCode\nmodel2.save('CatsDogs_sequentialModel.h5')\nvgg16.save('CatsDogs_vgg16.h5')\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\n\nTest\n\n\nCode\nnew_image_path =\"data/test/cats/cat_355.jpg\"\nim=plt.imshow(np.asarray(Image.open(new_image_path)))\nim\n\nclass_labels = ['Cats', 'Dogs']\n\ndef preprocess_image(img_path):\n    img = load_img(img_path, target_size=(224, 224))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = img / 255.0  # Normalize the image\n    return img\n\n\n# Preprocess the new image\nnew_image = preprocess_image(new_image_path)\n\n# Extract features from the new image using VGG16 model\nnew_features = vgg16.predict(new_image)\n\n# Make prediction on the new image\nprediction = model2.predict(new_features)\nprint(prediction)\npredicted_label = np.argmax(prediction)\nprint(predicted_label)\nprint(\"Predicted label:\", class_labels[predicted_label])\n\n\n1/1 [==============================] - 0s 82ms/step\n1/1 [==============================] - 0s 9ms/step\n[[9.991073e-01 8.927198e-04]]\n0\nPredicted label: Cats\n\n\n\n\n\n\n\nCode\nnew_image_path =\"data/test/dogs/dog_302.jpg\"\nim=plt.imshow(np.asarray(Image.open(new_image_path)))\nim\n\nclass_labels = ['Cats', 'Dogs']\n\ndef preprocess_image(img_path):\n    img = load_img(img_path, target_size=(224, 224))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = img / 255.0  # Normalize the image\n    return img\n\n\n# Preprocess the new image\nnew_image = preprocess_image(new_image_path)\n\n# Extract features from the new image using VGG16 model\nnew_features = vgg16.predict(new_image)\n\n# Make prediction on the new image\nprediction = model2.predict(new_features)\nprint(prediction)\npredicted_label = np.argmax(prediction)\nprint(predicted_label)\nprint(\"Predicted label:\", class_labels[predicted_label])\n\n\n1/1 [==============================] - 0s 73ms/step\n1/1 [==============================] - 0s 9ms/step\n[[0.01044283 0.9895572 ]]\n1\nPredicted label: Dogs\n\n\n\n\n\n\n\nCode\nurl = \"https://images.freeimages.com/images/large-previews/3f8/dog-1383342.jpg\"\ntempfile_path =tempfile.mktemp()\nurllib.request.urlretrieve(url, tempfile_path)\n\n\nnew_image = load_img(tempfile_path, target_size=(224,224))\n\nim=plt.imshow(np.asarray(new_image))\nim\nnew_image = img_to_array(new_image)\nnew_image = np.expand_dims(new_image, axis=0)\nnew_image = new_image / 255.0  # Normalize the image\n\n# Extract features from the new image using VGG16 model\nnew_features = vgg16.predict(new_image)\n\n# Make prediction on the new image\nprediction = model2.predict(new_features)\nprint(prediction)\npredicted_label = np.argmax(prediction)\nprint(predicted_label)\nprint(\"Predicted label:\", class_labels[predicted_label])\n\n\n1/1 [==============================] - 0s 83ms/step\n1/1 [==============================] - 0s 9ms/step\n[[3.6323289e-04 9.9963677e-01]]\n1\nPredicted label: Dogs"
  },
  {
    "objectID": "posts/Cancer Text classification/Cancer text classification.html",
    "href": "posts/Cancer Text classification/Cancer text classification.html",
    "title": "Text classification",
    "section": "",
    "text": "Text classification with deep learning leverages the power of deep neural networks to automatically extract complex features from textual data. By using techniques like recurrent neural networks (RNNs), convolutional neural networks (CNNs), or transformer models, deep learning enables accurate and efficient categorization of text into predefined classes, making it a robust and widely adopted approach in natural language processing tasks.\nFor this tutorial we will use tha cancer data from Kaggle. Based on biomedical text documentation, I will train a neural network to predict the type of cancer.\n\nImport libraries\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport collections\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n\n\n\nRead data\nThe dataset are composed of two columns: - Type: Cancer name (Thyroid_Cancer, Colon_Cancer, Lung_Cancer) - Text: Description of the cancer\n\n\nCode\ndata = pd.read_csv('data/alldata_1_for_kaggle.csv', sep=','\n                   ,header=0, encoding='mac_latin2',\n                   usecols=[1,2],names=['Type', 'Text'])\ndata.head(10)\n\n\n\n\n\n\n  \n    \n      \n      Type\n      Text\n    \n  \n  \n    \n      0\n      Thyroid_Cancer\n      Thyroid surgery in  children in a single insti...\n    \n    \n      1\n      Thyroid_Cancer\n      \" The adopted strategy was the same as that us...\n    \n    \n      2\n      Thyroid_Cancer\n      coronary arterybypass grafting thrombosis Ô¨Āb...\n    \n    \n      3\n      Thyroid_Cancer\n      Solitary plasmacytoma SP of the skull is an u...\n    \n    \n      4\n      Thyroid_Cancer\n      This study aimed to investigate serum matrix ...\n    \n    \n      5\n      Thyroid_Cancer\n      This study was performed to explore the effec...\n    \n    \n      6\n      Thyroid_Cancer\n      This study was performed assess the clinical ...\n    \n    \n      7\n      Thyroid_Cancer\n      Journal of International Medical Research ď Th...\n    \n    \n      8\n      Thyroid_Cancer\n      Gastric cancer GC persists as a worldwide pub...\n    \n    \n      9\n      Thyroid_Cancer\n      Scars Burns  HealingVolume   ď  reuse guideli...\n    \n  \n\n\n\n\n\n\nCode\ndata['Type'].value_counts()\n\n\nType\nThyroid_Cancer    2810\nColon_Cancer      2580\nLung_Cancer       2180\nName: count, dtype: int64\n\n\n\n\nData preparation:\nBefore injecting texts into the model, we have to clean our data:\n1- Remove ponctuations\n2- Remove numbers\n3- Transform to lower case\n4- Remove stop words like an, on, at, …\n5- Remove spaces\nThis step will be time-consuming due to the lengthy text.\n\nstop words:\n\n\n\nCode\nstopwords.words('english')\n\n\n['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]\n\n\n\n\nCode\ndef transform(text):\n    # Remove ponctuations\n    text = re.compile('[%s]' % re.escape(string.punctuation)).sub('',text)\n    # Remove numbers\n    text = re.sub(r'[0-9]', ' ', text)\n    # To lower case\n    text = text.lower()\n    # Remove stop words\n    text = ' '.join([i for i in text.split() if i not in stopwords.words('english') and len(i)>2])\n    # Remove spaces\n    text = re.sub(r'\\s+', ' ', text)\n    return(text)\n\ndata['Text_cleaned'] = data['Text'].apply(lambda x: transform(x))\n\n\n\n\nCode\nprint('Before transform function:', end='\\n')\nprint(data.iloc[1,1], end='\\n\\n')\nprint('After transform function:', end='\\n')\nprint(data.iloc[1,2])\n\n\nBefore transform function:\n\" The adopted strategy was the same as that used in prior years [] and is based on four exclusive queries that return  four  disjoint  citation  subsets The first query QPub_plain is based on a plaintext search in PubMed titles and s using keywords The  second  query  QPub_indexed relies on the PubMed indexing scheme using MeSH terms and results are made exclusive of the previous set The third one QWoS_restricted is based on a plaintext search in WoS restricted to the two research areas úMedical InformaticsĚ and úHealth Care Sciences  ServicesĚ The fourth query QWoS_filtered is based on the same plaintext search used in WoS but filtered by nonrelevant research areas eg Archeology Dance Zoology etc and the two research areas of the previous query It is of note that the two WoS queries select  only  nonPubMedindexed  papers that are supposed to be caught by the two PubMed queriesA  first  review  of  the  four  subsets  of retrieved  citations  was  performed  by  the two section editors to select  candidate best papers Following the IMIA Yearbook protocol these candidate best papers were then individually reviewed and rated by both section editors the chief editor of the Decision Support section and external reviewers from the international Medical Informatics community Based on the reviewersô ratings and comments the Yearbook editorial committee then selected the best papers of the year in the decision support domainIMIA Yearbook of Medical Informatics                                    IMIA and Ge Thieme Verlag KG 0cReview Results The    literature  search  has  been  performed on January   A total of  unique references were obtained distributed as follows  for QPub_plain  for QPub_indexed  for QWoS_restricted and  for QWoS_filtered yielding subtotals of  references from PubMed and  from WoS Compared to the previous year the global query retrieved   more  papers After  a  first  individual screening independently performed by both section editors based on the title and  of papers  not rejected by both section editors were discussed by the two editors to achieve a final selection of  candidate best  papers After  the  external  review  of these    s  the  editorial  committee finally selected three of them as best papers for  [ď] Table  They are discussed in the next section and summaries of their contents are available in the AppendixDiscussion and OutlookIn the first paper Hendriks  [] propose an approach to the modeling of clinical practice guidelines which certainly builds on already existing approaches but which is systematically conducted in order to be scalable and used to represent complex guidelines They promote the formalism of clinical decision trees CDTs as they are both clinically interpretable  by  healthcare  professionals and  computerinterpretable  thus  suitable for implementation in datadriven CDSSs The disambiguation of textual guidelines is supported first by the formal unequivocal specification of data items used as decision criteria using international coding systems to enforce interoperability and second by the representation of guideline knowledge as CDTs The method is applied to the Dutch breast cancer guidelines Sixty CDTs were built  involving  a  total  of    data  items among  which    could  not  be  linked  to standard terminologies The authors report the ambiguity of certain criteria which could be subjective or had multiple definitions The resulting knowledge base was implemented in a decision support application where it can be interactively browsed or automatically executed By modeling guidelines in such a way this work is a step forward in the sharing of encoded knowledgeIn the second paper KamiŇ°alińá  [] tackled the issues linked to the formalization of the medical processes used for managing chronic diseases and their execution in CDSSs They analyzed the decisionmaking dimensions of the therapeutic management of chronic diseases like those known to increase the cardiovascular risk and identified three basic levels therapy strategy dosage adaptation and intolerance management To handle these  different  aspects  consistently  they propose a formalism called extended Timed Transition Diagram eTTD With eTTDs they illustrate the multilevel and finegrained modeling required to capture the contents of arterial hypertension management guidelines This detailed demonstration on how procedural knowledge for hypertension management can be formalized to develop a CDSS could certainly be used in other medical domainsThe third paper by Khalifa  [] presents a conceptual and practical framework to help assess confidence in predictive tools GRASP  for  Grade  and Assess  Predictive Tools is both a method to look for evidence from the published literature and an analysis grid It standardizes the assessment of the available literature associated to a predictive tool and the grading of its level of proof Three phases of evaluation are considered i before the implementation of the tool to assess both its internal and external validity ii during the implementation to assess its potential effect and usability and iii after the implementation to assess its effectiveness and safety In each phase the level of evidence  can  be  assessed  from  the  study design A qualitative  summarizes the direction of evidence positive negative mixed This grid can be considered as similar to existing grids for instance the CONSORT statement for clinical trials However it gives a rigorous methodology for a critical appraisal of predictive tools and could be extended to all kind of CDSSs It might be a useful tool to extend the evidencebased culture in the field of medical informaticsBesides the three best papers selected for the  Decision  Support section  of  the   edition of the IMIA Yearbook several other works retrieved from the literature review deserve to be cited Some of them deal with the personalization of decisions Laleci   []  propose  a  scientific  and  technical approach to develop personalized care plans that comply with clinical practice guidelines for the management of complex polypathology situations Jafarpour  [] propose a solution to dynamically manage the conflicts that can rise in this type of complex contexts Ben Souissi  [] introduce the use of health  information  technology  involving multiple criteria decision to support the choice between antibiotics alternatives Interestingly other works promote the creation and sharing of operational knowledge bases as exemplified by Hendriks  [] Thus Huibers  [] transform the textual STOPPSTART criteria into unambiguous definitions mapped to  medical  terminologies  Canovas et  al [] formalize EUCAST expert rules as an ontology and production rules to detect antimicrobial therapies at risk of failure Mľller  [] propose an   diagnostic knowledge base that can compete with commercial ones Replacing humans is another topic of research and Spnig  [] work on two aspects to virtualize a doctor the automatic acquisition of data through sensors and speech recognition and the automation of diagnostic reasoning Rozenblum et al[] propose a machine learning method to generate clinically valid alerts to detect errors in prescriptions Acceptability  of  CDSS  is  another  key point Kannan  [] propose a method for a CDSS design to best meet a precisely specified and assessable user purpose Design alerts  may  also  avoid  rejection  of  CDSSs by caregivers Fernandes  [] created algorithms able to aggregate filter and reduce the notifications delivered to healthcare professionals Amrose et  al [] tried to understand in real life the impact of alerts on users and to find the actions they triggered Finally it is always interesting to obtain varied evaluation results of controversial CDSSs In this respect Kim  [] evaluated Watson for Oncology in thyroid carcinoma and reported a concordance rate with local practices considered as too low to adopt the tool As  evidenced  by  the  number  and  the variety of works around decision support research in the field is very active This yearôs selection highlighted pragmatic works that promote the transparency and sharing of the IMIA Yearbook of Medical Informatics 2020Duclos  0cTable     Best paper selection of s for the IMIA Yearbook of Medical Informatics  in the section 'Decision Support' The s are listed in alphabetical order of the first authorôs surname Section Decision Support\\uf0a7  Hendriks MP Verbeek XAAM van Vegchel T van der Sangen MJC Strobbe LJA Merkus JWS Zonderland HM Smorenburg CH Jager A Siesling S Transformation of the National Breast Cancer Guideline Into DataDriven Clinical Decision Trees JCO Clin Cancer Inform \\uf0a7\\t KamiŇ°alińá\\tA\\tRiaĪo\\tD\\tKert\\tS\\tWelzer\\tT\\tNemec\\tZlatolas\\tL\\tMultilevel\\tmedical\\tknowledge\\tformalization\\tto\\tsupport\\tmedical\\tpractice for chronic diseases Data  Knowledge Engineering ď\\uf0a7  Khalifa M Magrabi F Gallego B Developing a framework for evidencebased grading and assessment of predictive tools for clinical decision support BMC Med Inform Decis Mak  knowledge bases used by decision support tools as well as the grading of their utility The ultimate goal is that users could trust such tools to then use themAcknowledgementWe would like to thank all the present and past editorial boards of the IMIA Yearbook especially Martina Hutter and Adrien Ugon for their support as well as the reviewers for  their  participation  to  the  selection  of the  best  papers  for  the  Decision  Support section We cannot end this synopsis without a meaningful thought for our colleague and friend Vassilis  Koutkias  who  started  this year again to tackle the tasks of a Decision Support section coeditor but passed away in last December and unfortunately could not finishReferences  Jankovic I Chen JH Clinical Decision Support and Implications for the Clinician Burnout Crisis  Yearb Med Inform   Koutkias V Bouaud J Contributions on Clinical Decision Support from the  Literature Yearb Med Inform  Aug2811357  Hendriks MP Verbeek XAAM van Vegchel T van der Sangen MJC Strobbe LJA Merkus JWS  Transformation of the National Breast Cancer Guideline  Into  DataDriven  Clinical  Decision Trees JCO Clin Cancer Inform   KamiŇ°alińá A RiaĪo D Kert S Welzer T Nemec Zlatolas  L  Multilevel  medical  knowledge formalization  to  support  medical  practice  for chronic diseases Data  Knowledge Engineering ď  Khalifa  M  Magrabi  F  Gallego  B  Developing a  framework  for  evidencebased  grading  and assessment  of  predictive  tools  for  clinical  decision  support  BMC  Med  Inform  Decis  Mak    Laleci  GB Yuksel  M  Sarigul  B Arvanitis TN Lindman P Chen R  A Collaborative Platform for Management of Chronic Diseases via GuidelineDriven  Individualized  Care  Plans  Comput Struct Biotechnol J ď   Jafarpour  B  Raza Abidi  S Van  Woensel  W Raza Abidi  SS  Executiontime  integration  of clinical  practice  guidelines  to  provide  decision support for comorbid conditions Artif Intell Med   Ben Souissi S Abed M El Hiki L Fortemps P Pirlot  M  PARS  a  system  combining  semantic technologies with multiple criteria decision aiding for supporting antibiotic prescriptions J Biomed Inform   Huibers  CJA  Sallevelt  BTGM  de  Groot  DA Boer MJ van Campen JPCM Davids CJ  Conversion  of  STOPPSTART  version    into coded algorithms for software implementation A multidisciplinary consensus procedure Int J Med Inform  C°novasSegura B Morales A Juarez JM Campos M Palacios F Impact of expert knowledge on the detection of patients at risk of antimicrobial therapy failure by clinical decision support systems J Biomed Inform  Mľller L Gangadharaiah R Klein SC Perry J Bernstein G Nurkse D  An   access medical knowledge base for community driven diagnostic decision support system development BMC Med Inform Decis Mak  Spnig  S  EmbergerKlein A  Sowa  JP  Canbay A Menrad K Heider D The virtual doctor An interactive clinicaldecisionsupport system based on deep learning for noninvasive prediction of diabetes Artif Intell Med  Rozenblum R RodriguezMonguio R Volk LA Forsythe KJ Myers S McGurrin M  Using a Machine Learning System to Identify and Prevent Medication Prescribing Errors A Clinical and Cost Analysis Evaluation Jt Comm J Qual Patient Saf   Kannan V  Basit  MA  Bajaj  P  Carrington AR Donahue IB Flahaven EL  User stories as lightweight requirements for agile clinical decision support development J Am Med Inform Assoc  Fernandes CO Miles S Lucena CJP Cowan D Artificial  Intelligence Technologies  for  Coping with Alarm  Fatigue  in  Hospital  Environments Because  of  Sensory  Overload Algorithm  Development  and Validation  J  Med  Internet  Res 20192111e15406 Amroze A  Field TS  Fouayzi  H  Sundaresan D  Burns  L  Garber  L  et  al  Use  of  Electronic Health Record Access and Audit Logs to Identify Physician Actions Following Noninterruptive Alert  ing Descriptive Study JMIR Med Inform 201971e12650 Kim M Kim BH Kim JM Kim EH Kim K Pak K  Concordance in postsurgical radioactive iodine therapy recommendations between Watson for  Oncology  and  clinical  practice  in  patients with  differentiated  thyroid  carcinoma Cancer Correspondence toPr Catherine DuclosLIMICS INSERM Facult© L©onard de Vinci rue Marcel Cachin  Bobigny FranceEmail catherineduclosaphpfr IMIA Yearbook of Medical Informatics 2020Pragmatic Considerations on Clinical Decision Support from the  Literature 0cAppendix Content Summaries of Best Papers for the Decision Support Section of the  IMIA YearbookHendriks MP Verbeek XAAM van Vegchel T van der Sangen MJC Strobbe LJA Merkus JWS Zonderland HM Smorenburg CH Jager A Siesling STransformation of the National Breast Cancer Guideline into datadriven clinical decision treesJCO Clin Cancer Inform  May3114Since  clinical  practice  guidelines  are  still narrative and described in large textual documents the aim of this work was to model complex guidelines as datadriven clinical decision trees CDTs that could be still humaninterpretable while computerinterpretable for implementation in decision support systems The Dutch national breast cancer guidelines were translated into CDTs Data items  which  characterize  the  patient  and the tumor and represent decisional criteria were encoded unambiguously using existing classifications and coding systems related to breast cancer when feasible In total  CDTs  were  necessary  to  cover  the  whole guidelines driven by  data items Of all data items   could be coded using existing classification and coding systems All  CDTs represented  unique patient subpopulations Complex guidelines could be transformed as systematically constructed modular datadriven CDTs that are clinically interpretable and executable in a decision support applicationKamiŇ°alińá A RiaĪo D Kert S Welzer T Nemec Zlatolas LMultilevel medical knowledge formalization to support medical practice for chronic diseasesData  Knowledge Engineering  ďThis research is focused on knowledge representation to support the medical processes involved in chronic diseases management which can be viewed as a procedural and sequential  application  of  knowledge An intuitive easy and effective mechanism for medical  knowledge  formalization  is  proposed through a formalism called extended Timed Transition  Diagram  eTTD This formalism allows for the consistent representation of three basic levels of decision making that should be taken into account in the prescription and adaptation of longterm treatment therapy strategy dosage and intolerances The methodology can be manually applied to build eTTDs from clinical practice guidelines eTTDs implementation is demonstrated by modeling clinical practice guidelines for the therapeutic management of arterial hypertension The obtained models can be used as a baseline framework for the development of decision support systems involving medical proceduresKhalifa M Magrabi F Gallego BDeveloping a framework for evidencebased grading and assessment of predictive tools for clinical decision supportBMC Med Inform Decis Mak  Oct Deciding to choose a clinical predictive tool in clinical practice should be guided by its correctly assessed effectiveness The objective of this work is to developp a conceptual and practical framework to Grade and Assess Predictive tools GRASP and provide clinicians with a standardised evidencebased system to support their search for and selection of efficient predictive tools The GRASP framework grades predictive tools based on published evidence across three dimensions phase of evaluation level of evidence and direction of evidence The final grade of the tool is based on the phase of evaluation that gets  the  hightest  grade  supported  by  the highest level of positive or mixed evidence that  supports  a  positive   This framework was successfully applied to five predictive  tools  GRASP  report  updates could be a way to maintain a data base that documents the evidence of predictive tools IMIA Yearbook of Medical Informatics 2020Duclos  0c\"\n\nAfter transform function:\nadopted strategy used prior years based four exclusive queries return four disjoint citation subsets first query qpubplain based plaintext search pubmed titles using keywords second query qpubindexed relies pubmed indexing scheme using mesh terms results made exclusive previous set third one qwosrestricted based plaintext search wos restricted two research areas úmedical informaticsě úhealth care sciences servicesě fourth query qwosfiltered based plaintext search used wos filtered nonrelevant research areas archeology dance zoology etc two research areas previous query note two wos queries select nonpubmedindexed papers supposed caught two pubmed queriesa first review four subsets retrieved citations performed two section editors select candidate best papers following imia yearbook protocol candidate best papers individually reviewed rated section editors chief editor decision support section external reviewers international medical informatics community based reviewersô ratings comments yearbook editorial committee selected best papers year decision support domainimia yearbook medical informatics imia thieme verlag creview results literature search performed january total unique references obtained distributed follows qpubplain qpubindexed qwosrestricted qwosfiltered yielding subtotals references pubmed wos compared previous year global query retrieved papers first individual screening independently performed section editors based title papers rejected section editors discussed two editors achieve final selection candidate best papers external review editorial committee finally selected three best papers table discussed next section summaries contents available appendixdiscussion outlookin first paper hendriks propose approach modeling clinical practice guidelines certainly builds already existing approaches systematically conducted order scalable used represent complex guidelines promote formalism clinical decision trees cdts clinically interpretable healthcare professionals computerinterpretable thus suitable implementation datadriven cdsss disambiguation textual guidelines supported first formal unequivocal specification data items used decision criteria using international coding systems enforce interoperability second representation guideline knowledge cdts method applied dutch breast cancer guidelines sixty cdts built involving total data items among could linked standard terminologies authors report ambiguity certain criteria could subjective multiple definitions resulting knowledge base implemented decision support application interactively browsed automatically executed modeling guidelines way work step forward sharing encoded knowledgein second paper kamiň°alińá tackled issues linked formalization medical processes used managing chronic diseases execution cdsss analyzed decisionmaking dimensions therapeutic management chronic diseases like known increase cardiovascular risk identified three basic levels therapy strategy dosage adaptation intolerance management handle different aspects consistently propose formalism called extended timed transition diagram ettd ettds illustrate multilevel finegrained modeling required capture contents arterial hypertension management guidelines detailed demonstration procedural knowledge hypertension management formalized develop cdss could certainly used medical domainsthe third paper khalifa presents conceptual practical framework help assess confidence predictive tools grasp grade assess predictive tools method look evidence published literature analysis grid standardizes assessment available literature associated predictive tool grading level proof three phases evaluation considered implementation tool assess internal external validity implementation assess potential effect usability iii implementation assess effectiveness safety phase level evidence assessed study design qualitative summarizes direction evidence positive negative mixed grid considered similar existing grids instance consort statement clinical trials however gives rigorous methodology critical appraisal predictive tools could extended kind cdsss might useful tool extend evidencebased culture field medical informaticsbesides three best papers selected decision support section edition imia yearbook several works retrieved literature review deserve cited deal personalization decisions laleci propose scientific technical approach develop personalized care plans comply clinical practice guidelines management complex polypathology situations jafarpour propose solution dynamically manage conflicts rise type complex contexts ben souissi introduce use health information technology involving multiple criteria decision support choice antibiotics alternatives interestingly works promote creation sharing operational knowledge bases exemplified hendriks thus huibers transform textual stoppstart criteria unambiguous definitions mapped medical terminologies canovas formalize eucast expert rules ontology production rules detect antimicrobial therapies risk failure mľller propose diagnostic knowledge base compete commercial ones replacing humans another topic research spnig work two aspects virtualize doctor automatic acquisition data sensors speech recognition automation diagnostic reasoning rozenblum propose machine learning method generate clinically valid alerts detect errors prescriptions acceptability cdss another key point kannan propose method cdss design best meet precisely specified assessable user purpose design alerts may also avoid rejection cdsss caregivers fernandes created algorithms able aggregate filter reduce notifications delivered healthcare professionals amrose tried understand real life impact alerts users find actions triggered finally always interesting obtain varied evaluation results controversial cdsss respect kim evaluated watson oncology thyroid carcinoma reported concordance rate local practices considered low adopt tool evidenced number variety works around decision support research field active yearôs selection highlighted pragmatic works promote transparency sharing imia yearbook medical informatics duclos ctable best paper selection imia yearbook medical informatics section decision support listed alphabetical order first authorôs surname section decision supportuf hendriks verbeek xaam van vegchel van der sangen mjc strobbe lja merkus jws zonderland smorenburg jager siesling transformation national breast cancer guideline datadriven clinical decision trees jco clin cancer inform kamiň°alińátatriaīotdtkerttstwelzertttnemectzlatolastltmultileveltmedicaltknowledgetformalizationttotsupporttmedicaltpractice chronic diseases data knowledge engineering ďuf khalifa magrabi gallego developing framework evidencebased grading assessment predictive tools clinical decision support bmc med inform decis mak knowledge bases used decision support tools well grading utility ultimate goal users could trust tools use themacknowledgementwe would like thank present past editorial boards imia yearbook especially martina hutter adrien ugon support well reviewers participation selection best papers decision support section cannot end synopsis without meaningful thought colleague friend vassilis koutkias started year tackle tasks decision support section coeditor passed away last december unfortunately could finishreferences jankovic chen clinical decision support implications clinician burnout crisis yearb med inform koutkias bouaud contributions clinical decision support literature yearb med inform aug hendriks verbeek xaam van vegchel van der sangen mjc strobbe lja merkus jws transformation national breast cancer guideline datadriven clinical decision trees jco clin cancer inform kamiň°alińá riaīo kert welzer nemec zlatolas multilevel medical knowledge formalization support medical practice chronic diseases data knowledge engineering khalifa magrabi gallego developing framework evidencebased grading assessment predictive tools clinical decision support bmc med inform decis mak laleci yuksel sarigul arvanitis lindman chen collaborative platform management chronic diseases via guidelinedriven individualized care plans comput struct biotechnol jafarpour raza abidi van woensel raza abidi executiontime integration clinical practice guidelines provide decision support comorbid conditions artif intell med ben souissi abed hiki fortemps pirlot pars system combining semantic technologies multiple criteria decision aiding supporting antibiotic prescriptions biomed inform huibers cja sallevelt btgm groot boer van campen jpcm davids conversion stoppstart version coded algorithms software implementation multidisciplinary consensus procedure int med inform c°novassegura morales juarez campos palacios impact expert knowledge detection patients risk antimicrobial therapy failure clinical decision support systems biomed inform mľller gangadharaiah klein perry bernstein nurkse access medical knowledge base community driven diagnostic decision support system development bmc med inform decis mak spnig embergerklein sowa canbay menrad heider virtual doctor interactive clinicaldecisionsupport system based deep learning noninvasive prediction diabetes artif intell med rozenblum rodriguezmonguio volk forsythe myers mcgurrin using machine learning system identify prevent medication prescribing errors clinical cost analysis evaluation comm qual patient saf kannan basit bajaj carrington donahue flahaven user stories lightweight requirements agile clinical decision support development med inform assoc fernandes miles lucena cjp cowan artificial intelligence technologies coping alarm fatigue hospital environments sensory overload algorithm development validation med internet res amroze field fouayzi sundaresan burns garber use electronic health record access audit logs identify physician actions following noninterruptive alert ing descriptive study jmir med inform kim kim kim kim kim pak concordance postsurgical radioactive iodine therapy recommendations watson oncology clinical practice patients differentiated thyroid carcinoma cancer correspondence topr catherine ducloslimics inserm facult© l©onard vinci rue marcel cachin bobigny franceemail catherineduclosaphpfr imia yearbook medical informatics pragmatic considerations clinical decision support literature cappendix content summaries best papers decision support section imia yearbookhendriks verbeek xaam van vegchel van der sangen mjc strobbe lja merkus jws zonderland smorenburg jager siesling stransformation national breast cancer guideline datadriven clinical decision treesjco clin cancer inform may since clinical practice guidelines still narrative described large textual documents aim work model complex guidelines datadriven clinical decision trees cdts could still humaninterpretable computerinterpretable implementation decision support systems dutch national breast cancer guidelines translated cdts data items characterize patient tumor represent decisional criteria encoded unambiguously using existing classifications coding systems related breast cancer feasible total cdts necessary cover whole guidelines driven data items data items could coded using existing classification coding systems cdts represented unique patient subpopulations complex guidelines could transformed systematically constructed modular datadriven cdts clinically interpretable executable decision support applicationkamiň°alińá riaīo kert welzer nemec zlatolas lmultilevel medical knowledge formalization support medical practice chronic diseasesdata knowledge engineering ďthis research focused knowledge representation support medical processes involved chronic diseases management viewed procedural sequential application knowledge intuitive easy effective mechanism medical knowledge formalization proposed formalism called extended timed transition diagram ettd formalism allows consistent representation three basic levels decision making taken account prescription adaptation longterm treatment therapy strategy dosage intolerances methodology manually applied build ettds clinical practice guidelines ettds implementation demonstrated modeling clinical practice guidelines therapeutic management arterial hypertension obtained models used baseline framework development decision support systems involving medical procedureskhalifa magrabi gallego bdeveloping framework evidencebased grading assessment predictive tools clinical decision supportbmc med inform decis mak oct deciding choose clinical predictive tool clinical practice guided correctly assessed effectiveness objective work developp conceptual practical framework grade assess predictive tools grasp provide clinicians standardised evidencebased system support search selection efficient predictive tools grasp framework grades predictive tools based published evidence across three dimensions phase evaluation level evidence direction evidence final grade tool based phase evaluation gets hightest grade supported highest level positive mixed evidence supports positive framework successfully applied five predictive tools grasp report updates could way maintain data base documents evidence predictive tools imia yearbook medical informatics duclos\n\n\n\n\nWordCloud plots\nI can’t resist to draw some wordcloud.\nA word cloud plot is a popular visualization technique that displays the most frequently occurring words in a text corpus. It visually represents the importance of words by varying the size of each word according to its frequency, offering a quick and insightful overview of the most prominent terms within the given dataset.\nWith all the type of cancer:\n\n\nCode\ntext = ' '.join(i for i in data['Text_cleaned'])\nwords = text.split()\nword_counts_dict=collections.Counter(words)\nwordcloud=WordCloud(width=1200, height=800, background_color='white', max_words=200)\nwordcloud.generate_from_frequencies(word_counts_dict)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\n\n\n\n\nThyroid cancer:\n\n\nCode\ntext = ' '.join(i for i in data[data['Type']=='Thyroid_Cancer']['Text_cleaned'])\nwords = text.split()\nword_counts_dict=collections.Counter(words)\nwordcloud=WordCloud(width=1200, height=800, background_color='white', max_words=200)\nwordcloud.generate_from_frequencies(word_counts_dict)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\n\n\n\n\nCancer of colon:\n\n\nCode\ntext = ' '.join(i for i in data[data['Type']=='Colon_Cancer']['Text_cleaned'])\nwords = text.split()\nword_counts_dict=collections.Counter(words)\nwordcloud=WordCloud(width=1200, height=800, background_color='white', max_words=200)\nwordcloud.generate_from_frequencies(word_counts_dict)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\n\n\n\n\nLung cancer:\n\n\nCode\ntext = ' '.join(i for i in data[data['Type']=='Lung_Cancer']['Text_cleaned'])\nwords = text.split()\nword_counts_dict=collections.Counter(words)\nwordcloud=WordCloud(width=1200, height=800, background_color='white', max_words=200)\nwordcloud.generate_from_frequencies(word_counts_dict)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\nAs we can see on the previous plot, some words like Cancer, cell and patient are frequently used with all the cancer type. I have to use more than 200 words if I want a good accuracy of my model.\n\n\n\nModelization\nBefore setting the model, I splitted my data in two: - train: to train the model - test: to test the accuracy of the model.\nAfter that, we’ll use this technics: - Tokenizer: to transform list of words into list of integers as keras is dealing only with numbers. - Padding: to transform list of numbers into 2d numpy array with the same size (longest sequence in the list) - Label encoder: to transform labels (cancer type) to integers\n\n\nCode\nlabel_encoder = LabelEncoder()\nx_train, x_test, y_train, y_test =train_test_split(data['Text_cleaned'], data['Type'], test_size=0.2, random_state=42)\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(data['Text_cleaned'])\n\nmax_seq_length = 1000\nx_train_seq = tokenizer.texts_to_sequences(x_train)\nx_train_seq = pad_sequences(x_train_seq, maxlen=max_seq_length)\nx_test_seq = tokenizer.texts_to_sequences(x_test)\nx_test_seq = pad_sequences(x_test_seq, maxlen=max_seq_length)\n\ny_train_enc = label_encoder.fit_transform(y_train)\ny_test_enc = label_encoder.fit_transform(y_test)\n\n\nAs input, the first layer will be an embedding with: - input dimensions of 5000 as we’ll use the 5000 words most frequent - output dimensions are 256 - input length is 1000 (sentence’s length is 1000)\nThe model starts with an Embedding layer, followed by a Dropout layer to prevent overfitting.\nNext, a Conv1D layer with MaxPooling1D is used to capture local patterns in the text. A Bidirectional LSTM layer is incorporated to capture contextual information from both directions. BatchNormalization and additional Dropout layers are added to improve training stability and generalization.\nFinally, the model has a dense output layer with softmax activation for multi-class classification. The model is compiled using the Adam optimizer with a learning rate of 1e-4 and sparse categorical cross-entropy loss for the specified metrics of accuracy.\n\n\nCode\nmodel=Sequential()\nmodel.add(Embedding(input_dim=5000, output_dim=256, input_length=1000))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Bidirectional(LSTM(units=64)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=128, activation='selu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3, activation=\"softmax\"))\nmodel.summary()\n\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 1000, 256)         1280000   \n                                                                 \n dropout (Dropout)           (None, 1000, 256)         0         \n                                                                 \n conv1d (Conv1D)             (None, 998, 64)           49216     \n                                                                 \n max_pooling1d (MaxPooling1D  (None, 499, 64)          0         \n )                                                               \n                                                                 \n bidirectional (Bidirectiona  (None, 128)              66048     \n l)                                                              \n                                                                 \n batch_normalization (BatchN  (None, 128)              512       \n ormalization)                                                   \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense (Dense)               (None, 128)               16512     \n                                                                 \n batch_normalization_1 (Batc  (None, 128)              512       \n hNormalization)                                                 \n                                                                 \n dropout_2 (Dropout)         (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 3)                 387       \n                                                                 \n=================================================================\nTotal params: 1,413,187\nTrainable params: 1,412,675\nNon-trainable params: 512\n_________________________________________________________________\n\n\n\n\nCode\ny_train_enc = y_train_enc.astype(float)\nhistory = model.fit(x_train_seq, y_train_enc, validation_split=0.2, batch_size=100, epochs=15, verbose=1)\n\n\nEpoch 1/15\n\n\n2023-07-31 11:50:45.465002: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n\n\n49/49 [==============================] - 26s 497ms/step - loss: 1.0103 - accuracy: 0.5772 - val_loss: 1.0668 - val_accuracy: 0.4868\nEpoch 2/15\n49/49 [==============================] - 27s 550ms/step - loss: 0.5593 - accuracy: 0.7779 - val_loss: 1.0341 - val_accuracy: 0.3927\nEpoch 3/15\n49/49 [==============================] - 27s 559ms/step - loss: 0.2615 - accuracy: 0.9147 - val_loss: 0.9631 - val_accuracy: 0.4909\nEpoch 4/15\n49/49 [==============================] - 26s 527ms/step - loss: 0.1250 - accuracy: 0.9649 - val_loss: 0.8665 - val_accuracy: 0.6832\nEpoch 5/15\n49/49 [==============================] - 25s 508ms/step - loss: 0.0757 - accuracy: 0.9802 - val_loss: 0.7447 - val_accuracy: 0.8177\nEpoch 6/15\n49/49 [==============================] - 24s 483ms/step - loss: 0.0532 - accuracy: 0.9862 - val_loss: 0.5993 - val_accuracy: 0.9307\nEpoch 7/15\n49/49 [==============================] - 26s 526ms/step - loss: 0.0418 - accuracy: 0.9884 - val_loss: 0.4467 - val_accuracy: 0.9827\nEpoch 8/15\n49/49 [==============================] - 28s 569ms/step - loss: 0.0331 - accuracy: 0.9915 - val_loss: 0.3032 - val_accuracy: 0.9934\nEpoch 9/15\n49/49 [==============================] - 26s 535ms/step - loss: 0.0252 - accuracy: 0.9936 - val_loss: 0.1931 - val_accuracy: 0.9926\nEpoch 10/15\n49/49 [==============================] - 27s 545ms/step - loss: 0.0230 - accuracy: 0.9936 - val_loss: 0.1076 - val_accuracy: 0.9959\nEpoch 11/15\n49/49 [==============================] - 26s 525ms/step - loss: 0.0228 - accuracy: 0.9938 - val_loss: 0.0586 - val_accuracy: 0.9934\nEpoch 12/15\n49/49 [==============================] - 27s 560ms/step - loss: 0.0231 - accuracy: 0.9934 - val_loss: 0.0428 - val_accuracy: 0.9942\nEpoch 13/15\n49/49 [==============================] - 27s 556ms/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 0.0228 - val_accuracy: 0.9950\nEpoch 14/15\n49/49 [==============================] - 26s 524ms/step - loss: 0.0176 - accuracy: 0.9955 - val_loss: 0.0266 - val_accuracy: 0.9942\nEpoch 15/15\n49/49 [==============================] - 26s 536ms/step - loss: 0.0165 - accuracy: 0.9932 - val_loss: 0.0099 - val_accuracy: 0.9975\n\n\n\n\nCode\n# Get the training and validation accuracy\ntraining_accuracy = history.history['accuracy']\nvalidation_accuracy = history.history['val_accuracy']\n\n# Get the training and validation loss\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\n# Plot the accuracy\nplt.figure(figsize=(8, 4))\nplt.plot(training_accuracy, label='Training Accuracy')\nplt.plot(validation_accuracy, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n# Plot the loss\nplt.figure(figsize=(8, 4))\nplt.plot(training_loss, label='Training Loss')\nplt.plot(validation_loss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAccuracy of trained model:\nAll right we have trained our model, now let’s apply it to the test dataset and display the accuracy:\n\n\nCode\naccuracy = model.evaluate(x_test_seq, y_test_enc)\nprint(f\"Accuracy of the trained model: {(100*accuracy[1]):.2f} %\")\n\n\n48/48 [==============================] - 2s 36ms/step - loss: 0.0144 - accuracy: 0.9941\nAccuracy of the trained model: 99.41 %\n\n\n\nConfusion matrix\nA confusion matrix is a fundamental tool in the field of machine learning and classification tasks.\nIt provides a comprehensive and intuitive representation of the performance of a classification model by comparing its predicted class labels with the actual ground truth labels\n\n\nCode\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\ny_prediction = np.argmax(model.predict(x_test_seq), axis=1)\nconf_matrix = confusion_matrix(y_true=y_test_enc, y_pred=y_prediction)\nclass_labels = ['Thyroid_Cancer', 'Colon_Cancer', 'Lung_Cancer']\nConfusionMatrixDisplay(conf_matrix, display_labels=class_labels).plot()\n\n\n48/48 [==============================] - 2s 35ms/step\n\n\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2d40f0460>"
  }
]